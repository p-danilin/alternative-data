{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import yfinance as yf\n",
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"FINNHUB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_NAME = 'stocks_data.db'\n",
    "conn = sqlite3.connect(DATABASE_NAME)\n",
    "print(f\"Connected to database: {DATABASE_NAME}\")\n",
    "c = conn.cursor()\n",
    "\n",
    "DEFAULT_START_DATE = pd.to_datetime(\"2023-08-01\")\n",
    "c.execute('''\n",
    "CREATE TABLE IF NOT EXISTS stock_data_combined (\n",
    "    ticker TEXT, \n",
    "    date DATE, \n",
    "    open REAL, \n",
    "    high REAL, \n",
    "    low REAL, \n",
    "    close REAL, \n",
    "    volume INTEGER, \n",
    "    dividends REAL, \n",
    "    stock_splits REAL, \n",
    "    eps_estimate REAL,\n",
    "    reported_eps REAL,\n",
    "    surprise_percentage REAL,\n",
    "    change INTEGER, \n",
    "    transactionPrice REAL, \n",
    "    number_of_buys INTEGER,\n",
    "    number_of_sells INTEGER,\n",
    "    number_of_gifts INTEGER\n",
    ")''')\n",
    "print(\"Table 'stock_data_combined' creation command executed.\")\n",
    "\n",
    "c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = c.fetchall()\n",
    "if ('stock_data_combined',) in tables:\n",
    "    print(\"'stock_data_combined' table exists.\")\n",
    "else:\n",
    "    print(\"'stock_data_combined' table does not exist.\")\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_for_period(stock, from_date, to_date, api_key):\n",
    "    time.sleep(1) \n",
    "    \n",
    "    from_date = pd.to_datetime(from_date)\n",
    "    to_date = pd.to_datetime(to_date)\n",
    "    url = f'https://finnhub.io/api/v1/stock/insider-transactions?symbol={stock}&from={from_date.strftime(\"%Y-%m-%d\")}&to={to_date.strftime(\"%Y-%m-%d\")}&token={api_key}'\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "\n",
    "    all_data = pd.DataFrame() \n",
    "\n",
    "    if 'data' not in data or len(data['data']) < 2000:\n",
    "        return pd.DataFrame(data=data.get('data', []))\n",
    "    else:\n",
    "        middle_date = from_date + (to_date - from_date) / 2\n",
    "        first_half = fetch_data_for_period(stock, from_date, middle_date.strftime(\"%Y-%m-%d\"), api_key)\n",
    "        second_half = fetch_data_for_period(stock, middle_date.strftime(\"%Y-%m-%d\"), to_date, api_key)\n",
    "        all_data = pd.concat([all_data, first_half, second_half])\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def fetch_data_from_api(stock, start_date, end_date=None, api_key=None):\n",
    "    if end_date is None:\n",
    "        end_date = pd.to_datetime(\"today\")\n",
    "    else:\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "\n",
    "    all_data = fetch_data_for_period(stock, start_date, end_date, api_key)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def fetch_stock_data(ticker_symbol, start_date=None, end_date=None):\n",
    "    # Fetching the ticker data\n",
    "    ticker_data = yf.Ticker(ticker_symbol)\n",
    "    historical_data = ticker_data.history(period=\"max\")\n",
    "    historical_data.index = historical_data.index.normalize()\n",
    "    earnings_dates_data = ticker_data.get_earnings_dates(limit=200)\n",
    "    earnings_dates_data.index = earnings_dates_data.index.normalize()\n",
    "\n",
    "    # If start_date and/or end_date is provided, filter the historical_data\n",
    "    if start_date:\n",
    "        # Ensure start_date is tz-aware\n",
    "        start_date = pd.Timestamp(start_date).tz_localize(historical_data.index.tz).normalize()\n",
    "        historical_data = historical_data[historical_data.index >= start_date]\n",
    "\n",
    "    if end_date:\n",
    "        # Ensure end_date is tz-aware\n",
    "        end_date = pd.Timestamp(end_date).tz_localize(historical_data.index.tz).normalize()\n",
    "        historical_data = historical_data[historical_data.index <= end_date]\n",
    "\n",
    "    # Merging data\n",
    "    combined_data = historical_data.merge(earnings_dates_data, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "\n",
    "def get_missing_date_ranges(conn, stock, start_date=None):\n",
    "    # Query the database to get all dates for which we have data for the stock\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT date FROM stock_data_combined WHERE ticker = ?', (stock,))\n",
    "    dates_in_db = c.fetchall()\n",
    "    dates_in_db = [pd.to_datetime(date[0]) for date in dates_in_db if date[0] is not None]\n",
    "\n",
    "    # Use the provided start_date or DEFAULT_START_DATE\n",
    "    if not start_date:\n",
    "        start_date = DEFAULT_START_DATE\n",
    "    start_date = pd.to_datetime(start_date).tz_localize(None)\n",
    "\n",
    "    # All business days from start_date to today\n",
    "    end_date = pd.to_datetime(\"today\")\n",
    "    all_dates = pd.date_range(start_date, end_date, freq='B')\n",
    "\n",
    "    # Detect the missing dates\n",
    "    missing_dates = [date for date in all_dates if date not in dates_in_db]\n",
    "\n",
    "    # Construct date ranges\n",
    "    date_ranges = []\n",
    "    if missing_dates:\n",
    "        start_range = missing_dates[0]\n",
    "        end_range = missing_dates[0]\n",
    "        \n",
    "        for i in range(1, len(missing_dates)):\n",
    "            if missing_dates[i] == end_range + pd.Timedelta(days=1):\n",
    "                # Extend the range\n",
    "                end_range = missing_dates[i]\n",
    "            else:\n",
    "                # Save the previous range and start a new one\n",
    "                date_ranges.append((start_range, end_range))\n",
    "                start_range = missing_dates[i]\n",
    "                end_range = missing_dates[i]\n",
    "                \n",
    "        # Add the last detected range\n",
    "        date_ranges.append((start_range, end_range))\n",
    "\n",
    "    return date_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the api\n",
    "# print(fetch_data_from_api('AAPL', '2023-08-01', api_key).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of all US stock tickers\n",
    "url = f'https://finnhub.io/api/v1/stock/symbol?exchange=US&token={api_key}'\n",
    "r = requests.get(url)\n",
    "data = json.loads(r.text)\n",
    "\n",
    "stock_tickers = [item['symbol'] for item in data]\n",
    "\n",
    "print(stock_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_tickers = [\"AAPL\", \"GILD\", \"TSLA\"]  # Temporary shorter list of tickers to fetch data for\n",
    "\n",
    "for stock in stock_tickers:\n",
    "    missing_ranges = get_missing_date_ranges(conn, stock)\n",
    "    # If no missing dates, continue to the next stock\n",
    "    if not missing_ranges:\n",
    "        continue\n",
    "    \n",
    "    for start_date, end_date in missing_ranges:\n",
    "        print(f\"Fetching data for {stock} from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}...\")\n",
    "        \n",
    "        insider_data = fetch_data_from_api(stock, start_date, end_date, api_key)\n",
    "        # Assign the stock ticker to the fetched insider data for identification\n",
    "        insider_data['ticker'] = stock\n",
    "\n",
    "        print(f\"Fetched {len(insider_data)} insider transactions for {stock}...\")\n",
    "\n",
    "        stock_data = fetch_stock_data(stock, start_date, end_date) \n",
    "        stock_data.index = stock_data.index.tz_localize(None)\n",
    "        \n",
    "        # Filter the stock data to only include entries from the start_date to end_date\n",
    "        stock_data = stock_data[start_date:end_date] \n",
    "        stock_data['ticker'] = stock\n",
    "        \n",
    "        print(f\"Fetched stock data for {stock} from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}...\")\n",
    "\n",
    "        # If insider_data isn't empty, compute the insider related columns\n",
    "        if not insider_data.empty:\n",
    "            insider_data['insiderPortfolioChange'] = insider_data['change'] / (insider_data['share'] - insider_data['change'])\n",
    "            conditions = [\n",
    "                (insider_data['change'] >= 0) & (insider_data['transactionPrice'] > 0),\n",
    "                (insider_data['change'] <= 0) & (insider_data['transactionPrice'] > 0),\n",
    "                (insider_data['transactionPrice'] == 0)\n",
    "            ]\n",
    "            values = ['Buy', 'Sale', 'Gift']\n",
    "            insider_data['buyOrSale'] = np.select(conditions, values)\n",
    "            insider_data['transactionDate'] = pd.to_datetime(insider_data['transactionDate'])\n",
    "\n",
    "            # Aggregate data based on transaction date\n",
    "            aggregated_data = insider_data.groupby('transactionDate').agg({\n",
    "                'change': 'sum',\n",
    "                'transactionPrice': 'mean',\n",
    "                'buyOrSale': lambda x: x.value_counts().to_dict()\n",
    "            }).reset_index()\n",
    "\n",
    "            # Split the buyOrSale dictionary column into separate columns\n",
    "            aggregated_data['Number of Buys'] = aggregated_data['buyOrSale'].apply(lambda x: x.get('Buy', 0))\n",
    "            aggregated_data['Number of Sells'] = aggregated_data['buyOrSale'].apply(lambda x: x.get('Sale', 0))\n",
    "            aggregated_data['Number of Gifts'] = aggregated_data['buyOrSale'].apply(lambda x: x.get('Gift', 0))\n",
    "            aggregated_data.drop('buyOrSale', axis=1, inplace=True)\n",
    "            # Convert 'transactionDate' column in all_data to datetime\n",
    "            aggregated_data['transactionDate'] = pd.to_datetime(aggregated_data['transactionDate'])\n",
    "            aggregated_data['transactionDate'] = aggregated_data['transactionDate'].dt.normalize()\n",
    "        else:\n",
    "            # Create an empty aggregated_data DataFrame with the necessary columns if insider_data is empty\n",
    "            aggregated_data = pd.DataFrame(columns=['transactionDate', 'change', 'transactionPrice', 'Number of Buys', 'Number of Sells', 'Number of Gifts'])\n",
    "\n",
    "        # Remove timezone from the stock_data index\n",
    "        stock_data.index = stock_data.index.tz_localize(None)\n",
    "        stock_data = stock_data.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "        # Merge on the 'date' column\n",
    "        combined_data = pd.merge(stock_data, aggregated_data, left_on='date', right_on='transactionDate', how='left')\n",
    "        combined_data['date'] = combined_data['date'].dt.normalize()\n",
    "        combined_data.drop('transactionDate', axis=1, inplace=True)  # Drop the redundant transactionDate column\n",
    "\n",
    "        # Renaming columns\n",
    "        renamed_columns = {\n",
    "            \"Open\": \"open\",\n",
    "            \"High\": \"high\",\n",
    "            \"Low\": \"low\",\n",
    "            \"Close\": \"close\",\n",
    "            \"Volume\": \"volume\",\n",
    "            \"Dividends\": \"dividends\",\n",
    "            \"Stock Splits\": \"stock_splits\",\n",
    "            \"EPS Estimate\": \"eps_estimate\",\n",
    "            \"Reported EPS\": \"reported_eps\",\n",
    "            \"Surprise(%)\": \"surprise_percentage\",\n",
    "            \"Number of Buys\": \"number_of_buys\",\n",
    "            \"Number of Sells\": \"number_of_sells\",\n",
    "            \"Number of Gifts\": \"number_of_gifts\"\n",
    "        }\n",
    "\n",
    "        # Write the combined data to the SQLite database\n",
    "        combined_data = combined_data.rename(columns=renamed_columns)\n",
    "        combined_data.to_sql('stock_data_combined', conn, if_exists='append', index=False)\n",
    "        # print(combined_data.head())\n",
    "    print(f\"Written combined data for {stock} to database.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deleting database records to see if the previous cell will find them and handle them\n",
    "c.execute(f\"DELETE FROM stock_data_combined WHERE date = '2023-08-30 00:00:00'\")\n",
    "c.execute(f\"DELETE FROM stock_data_combined WHERE date = '2023-08-24 00:00:00'\")\n",
    "c.execute(f\"DELETE FROM stock_data_combined WHERE date = '2023-08-25 00:00:00'\")\n",
    "c.execute(f\"DELETE FROM stock_data_combined WHERE date = '2023-08-01 00:00:00'\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query a few rows from the database to inspect the data\n",
    "c.execute('SELECT * FROM stock_data_combined LIMIT 2000')\n",
    "rows = c.fetchall()\n",
    "\n",
    "# Get column names from cursor description\n",
    "column_names = [description[0] for description in c.description]\n",
    "print(column_names)\n",
    "for row in rows:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
